\chapter{Methodology}\label{chap:methodology}

\section{Proposed evaluation protocol}\label{sec:proposed-evaluation-protocol}

In this section, we present a comprehensive evaluation protocol for assessing the performance of Structure from Motion (SfM) algorithms.
The proposed evaluation protocol consists of four main components: Camera Pose Evaluation, Novel View Synthesis Evaluation, Time performance, and GPU memory usage.
These components are designed to evaluate the accuracy of camera pose estimation and the quality of the reconstructed scene geometry, respectively.

\subsection{Camera Pose Evaluation}
To quantitatively assess camera pose accuracy, we compute relative rotations and translations between all possible pairs of estimated camera poses and compare these to the corresponding ground-truth poses.

Given two cameras \( i \) and \( j \), let the relative rotation and translation from camera \( i \) to camera \( j \) be defined as:
\begin{equation}
    R_{ij} = R_j R_i^T
\end{equation}
\begin{equation}
    \vec{t}_{ij} = \vec{t}_i - R_{ij} \vec{t}_j
\end{equation}

where \( R_i \) and \( R_j \) are the rotation matrices of cameras \( i \) and \( j \) respectively, and \( \vec{t}_i \) and \( \vec{t}_j \) are their respective translation vectors.

\paragraph{Relative Translation Error (RTE)}
The Relative Translation Error (RTE) measures the angular difference between the estimated and ground-truth camera relative translations and is computed as:

\begin{equation}
    \text{RTE}_{ij} = \arccos\left(\frac{\vec{t_{ij}}^{\text{gt}} \cdot \vec{t_{ij}}^{\text{est}}}{\lVert \vec{t_{ij}}^{\text{gt}} \rVert \lVert \vec{t_{ij}}^{\text{est}} \rVert}\right)
\end{equation}

\paragraph{Relative Rotation Error (RRE)}
The Relative Rotation Error (RRE) quantifies the angular difference between estimated and ground-truth relative rotations. It is defined as:

\begin{equation}
    \text{RRE}_{ij} = \arccos\left(\frac{\text{trace}(R_{ij}^{\text{gt}} (R_{ij}^{\text{est}})^T) - 1}{2}\right)
\end{equation}

\paragraph{Absolute Trajectory Error (ATE)}
TODO; Explain ATE and how it is computed, model alignment, etc.

\paragraph{Area Under the Curve (AUC)}

To summarize the performance of the SfM algorithms, we report the Area Under the Curve (AUC) for the cumulative distribution of angular errors at different thresholds. 
This provides a comprehensive assessment of the algorithm's accuracy across varying levels of tolerance.


\subsection{Novel View Synthesis Evaluation}
To evaluate the quality and accuracy of the reconstructed scene geometry, we employ Novel View Synthesis using Neural Radiance Fields (NeRF). 
Specifically, the \texttt{Nerfstudio} \cite{nerfstudio} framework is utilized to render synthetic images from novel camera viewpoints not included in the original set of images used for reconstruction. 
Several Neural Radiance Fields methods have been proposed over the recent years, but we focus on Nerfacto available by default in \texttt{Nerfstudio}.

The quality of these synthetic views is quantitatively assessed using standard metrics, including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). 

\begin{itemize}
    \item \textbf{PSNR}: Measures the ratio of peak signal power to noise power in the image, expressed in decibels (dB). Higher values indicate lower distortion.
    \item \textbf{SSIM}: Evaluates perceptual similarity by comparing luminance, contrast, and structure between images. A higher SSIM score implies a closer match to the reference image.
    \item \textbf{LPIPS}: Uses deep learning-based feature comparisons to assess perceptual similarity. Lower LPIPS scores indicate greater visual fidelity. By default, \texttt{Nerfstudio} uses AlexNet features for LPIPS computation.
\end{itemize}

Higher PSNR and SSIM scores, along with lower LPIPS scores, indicate superior reconstruction fidelity and scene representation accuracy.




\section{Dataset Selection}\label{sec:dataset-selection}

TODO: 

Explain each dataset used for evaluation, including the number of images, available ground-truth data, 
if the dataset has outdoor/indoors scenes, lighting conditions, if it is about a large scene with a long trajectory or if it turns around an main subject
ETH3D and LaMAR has a long trajectory that goes around a bulding wheras Tanks and Temple and MiPNeRF360 turns around a main subject.

AceZero has limitation, it can't handle dataset with different camera intrinsics. This is why we are not using dataset like IMC.


\subsection{ETH3D Stereo}


\subsection{LaMAR}


\subsection{MiPNeRF360}


\subsection{Tanks and Temple}