\chapter{Background and Related Work}\label{chap:related_work}

In this chapter, we provide a comprehensive overview of the foundational concepts and recent advancements relevant to Structure-from-Motion (SfM).
We begin by describing classical SfM techniques, highlighting traditional feature extraction, matching methods, and optimization strategies. 
Subsequently, we review emerging learning-based approaches, focusing on methods that leverage neural networks to enhance reconstruction accuracy, efficiency, and robustness. 
The chapter further delves into detailed discussions of specific state-of-the-art methods, including GLOMAP, VGGSfm, FlowMap, and Ace Zero, 
clarifying their claimed advantages and limitations compared to classical SfM pipelines like Colmap.

\section{Classical SfM}
SfM approaches can be broadly classified into two categories: incremental and global.

\subsection{Incremental SfM}
Incremental Structure-from-Motion (SfM) follows a sequential approach, where the 3D reconstruction is built progressively by adding one image at a time. 
This method begins with an initial pair of images and expands the reconstruction iteratively by registering new images and refining the existing structure. 
The core steps of incremental SfM include feature extraction and matching, relative camera pose estimation, and bundle adjustment.

\paragraph{Feature Extraction and Matching}
The first step in incremental SfM involves detecting and describing key points in each image. 
Traditional methods rely on hand-crafted feature descriptors like SIFT \cite{Lowe2004DistinctiveIF} or ORB \cite{rublee2011orb}, 
which are designed to extract distinctive image features that are robust to changes in scale, rotation, and lighting conditions.

Once features are detected, they are compared across overlaping images to establish correspondences.
This is typically done using nearest-neighbor matching, where the descriptor of a feature in one image is compared to all features in another image to find the best match.
This process can be computationally expensive, especially for large datasets, and is often optimized using techniques like FLANN \cite{muja2009fast} or approximate nearest neighbor search.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/related_work/matches.jpg}
    \caption{Example of feature matching between two images. The lines connect matched features across the two images.}
    \label{fig:incremental_sfm}
\end{figure}


\paragraph{Relative Camera Pose Estimation}
Given the matched feature correspondences between image pairs, the next step is to estimate the motion of the camera between the two views.
This is typically done by computing the essential or fundamental matrix, which encapsulates the relative rotation and translation between the two camera poses.

This is achieved using epipolar geometry constraints, often solved via the five-point or eight-point algorithms within a RANSAC \cite{fischler1981random} framework to remove matches that are outliers.
The estimated relative camera pose allows for triangulation, reconstructing initial 3D points.

\paragraph{Bundle Adjustment}
As new images are added, the estimated structure and camera parameters can suffers from drift and inaccuracies.
Bundle adjustment is a key step in incremental SfM that refines the camera poses and 3D structure by minimizing the reprojection error.
Non-linear least squares solvers such as Levenberg-Marquardt or Ceres solver are commonly used to achieve this refinement. 
Incremental SfM benefits from high accuracy but suffers from scalability issues as the number of images increases.

\subsection{Global SfM}
In contrast to incremental approaches, global SfM processes all images simultaneously to compute a globally consistent 3D structure.
Instead of reapeting the costly bundle adjustment, global SfM methods estimate camera geometry for all input images at once.

A \emph{view graph} is constructed with all image pairs and their estimated relative poses.

The graph is then used to estimate camera intrinsics, as well as construction of a relative camera pose graph.
The relative camera pose graph is then used to perform \emph{rotation averaging} as well as \emph{translation averaging} \cite{Chatterjee2013, theia-manual, moulon2016openmvg}
Finally, with both global rotations and translations estimated, a global bundle adjustment is performed to refine the camera parameters and 3D point positions, ensuring a consistent and accurate reconstruction of the scene.
This approach allow for more scalable reconstructions.

However, three limitations remains: \\
\textbf{Translation Scale Recovery}: The scale of the translations cannot be determined from two-view geometry alone, requiring additional constraints or prior information for absolute scale recovery.
The process of translation averaging is particularly sensitive to noise and outliers making this task challenging. \\
\textbf{Prior knowledge of camera intrinsics}: Global SfM methods typically require prior knowledge of camera intrinsics, which is not always available. \\
\textbf{Colinear motion}: When the motion of the camera is colinear, is lead to degenerate reconstruction problem. Which is challenging as such motion is frequently encountered in real-world scenarios (e.g., self-driving car or drone flying in a straight line).

\section{GLOMAP}\label{sec:glomap}

GLOMAP \cite{pan2024glomap} is a recent global Structure-from-Motion method that aims to overcome the challenges of global SFM. It can deal with unknown camera intrinsics as well as colinear motion.

\paragraph{Feature Extraction and Matching}
The pipeline rely of COLAMP features extraction and matching, which use RootSIFT \cite{arrandjelovic2012three} for features extraction and has several option for matching: 
\begin{itemize}
    \item \textbf{Exhaustive}: All images are matched against each other, which is computationally expensive but provides the most accurate matches.
    \item \textbf{Sequential}: Matches are computed in a sequential manner, which is more efficient than exhaustive matching but requires sequential images.
    \item \textbf{Vocabulary Tree}: A Bag-of-Words images retrival approach \cite{schoenberger2016vote} that allows to find overlaping images in a large dataset and match them.
\end{itemize}

\paragraph{Camera Pose Estimation and Triangulation}
Since GLOMAP is a global SfM method, it seeks to estimate all the camera rotation and translation parameters simultaneously.
The global camera pose estimation is performed usually performed by a combination of rotation averaging and translation averaging. 
However, due to noise and outliers as well as scale ambiguity, translation averaging is particularly challenging.

GLOMAP introduces a novel approach and perform rotation averaging and a steps called \emph{Global positioning} instead of translation averaging. \\
Global positioning is the key step for robustness and perform directly a joint camera and point position estimation. 
Employing the BATA loss \cite{zhuang2019baselinedesensitizingtranslationaveraging} with a bounded reprojection error in the range $[0, 1]$. 
This allows the optimization process to converge quickly and prevent outliers from impacting the results.

By avoiding the translation averaging step, GLOMAP is able to handle colinear motion and better handle outliers.

Finally, global bundle adjustment is performed to refine the camera parameters and 3D point positions, ensuring a consistent and accurate reconstruction of the scene.

The authors claim superior performance and accuracy compared to traditional global SfM methods, and on par with COLMAP, while being significantly faster.

A limitation of GLOMAP remains. If the scene has a rotational symmetry, the rotation averaging step collapses, making it a degenerate case for GLOMAP.

\section{VGGSfM}\label{sec:vggsfm}

Visual Geometry Grounded Deep Structure From Motion (VGGSfM) \cite{wang2023vggsfm} introduces a novel, fully differentiable pipeline for Structure-from-Motion (SfM) that harnesses the potential of deep learning. The authors propose a unified framework that integrates the entire SfM pipeline into a function $f_{\theta}$, parameterized by $\theta$, which maps a set of images $\mathcal{I}$ to camera parameters $\mathcal{P}$ and a 3D point cloud $\mathcal{X}$. Here, $\theta$ is a learnable parameter, optimized by minimizing a loss function $\mathcal{L}$.

In its original formulation, $f_\theta$ can be decomposed into four stages: (1) Point Tracking, (2) Camera Estimator, (3) Triangulator, and (4) Bundle Adjustment.

\paragraph{Point Tracking.}
Unlike traditional SfM methods that compute pairwise correspondences and chain them into multi-image tracks, VGGSfM employs a deep feed-forward tracker to predict pixel-accurate multi-image correspondences directly from the input images. 
It typically selects a set of query points in one image (e.g.\ using SuperPoint \cite{detone18superpoint}, SIFT \cite{Lowe2004DistinctiveIF}, ALIKED \cite{Zhao2023ALIKED}, or a combination) and extracts their feature descriptors. These descriptors are matched across all other images using a multi-scale cost volume, encoding their similarity. 
A Transformer attends to all input frames jointly, predicting the 2D location of each query point in every view along with its visibility and confidence. 
In contrast to pairwise matching that relies on incremental chaining, VGGSfM's direct multi-image tracking reduces both complexity and drift errors.

\paragraph{Camera Estimator and Triangulator.}
The initial camera estimator and triangulator stages use deep Transformers to generate an initial estimate of the camera parameters $\mathcal{P}$ and the 3D points $\mathcal{X}$, as opposed to relying on rotation/translation averaging or incremental registration. 
In the original paper's pipeline, the network could leverage track features as well as image features to produce camera poses. All cameras and points are thus registered collectively, ensuring differentiability and bypassing many incremental SfM complexities.

\paragraph{Bundle Adjustment.}
Finally, the pipeline employs a differentiable second-order Levenberg-Marquardt optimizer from the Theseus library \cite{pineda2022theseus} to refine both the camera parameters and 3D points by minimizing reprojection error. 
Because bundle adjustment (BA) is fully differentiable, the entire SfM pipeline can be trained end-to-end, unlike in traditional approaches that use non-differentiable solvers.

\paragraph{Comparison with Global SfM}
VGGSfM is related to global SfM approaches but differs in three crucial aspects:
\begin{itemize}
    \item It learns to track points directly from the images, rather than performing pairwise matching.
    \item It uses a deep network to regress camera poses in a single pass, rather than rotation/translation averaging.
    \item It employs differentiable bundle adjustment to refine parameters in an end-to-end manner.
\end{itemize}

\subsection*{VGGSfM v1.1 Updates}
Starting from v1.1, the VGGSfM implementation separates the camera predictor from the point-tracking stage. 
Concretely, the camera predictor now relies \emph{only} on image features rather than requiring track features. 
This design choice arose in response to users feedback:
\begin{itemize}
    \item \textbf{Faster Pose Estimates.} Some scenarios benefit from real-time or near-real-time camera pose estimation without waiting for the (comparatively slower) track prediction.
    \item \textbf{Flexibility.} Users can opt to generate approximate camera poses quickly and refine them later (if desired) with track-based bundle adjustment.
\end{itemize}
This change does slightly reduce the initial pose accuracy—because track features are no longer used for the first pose guess—but in practice, errors remain small enough (e.g.\ $\approx 3^\circ$ mean rotational error and $6\%$ translational error) \cite{VGGSFM_GitHubIssue}. 
These errors can be effectively corrected via the subsequent BA stage if a high-accuracy reconstruction is needed.


While VGGSfM demonstrates robust performance on several benchmarks such as Co3D \cite{reizenstein21co3d}, IMC \cite{Jin_2020}, and ETH3D \cite{schoeps2017cvpr}, the authors acknowledge it does not (yet) handle large-scale datasets (thousands of frames) as readily as classical SfM pipelines. Ongoing work (VGGSfM v2) aims to address these scalability challenges while maintaining the end-to-end differentiability and strong performance.

\section{FlowMap}\label{sec:flowmap}


FlowMap \cite{smith24flowmap} presented by C. Smith and D. Charatan et al is a recently introduced Structure-from-Motion method leveraging an end-to-end differentiable approach for estimating camera poses, intrinsics, and dense per-frame depth maps from video sequences. 
Unlike classical SfM methods such as COLMAP, FlowMap does not rely on explicit 3D point triangulation from sparse correspondences. 
Instead, it formulates SfM as a gradient-descent optimization problem, supervised purely by off-the-shelf optical flow and sparse point tracking correspondences.

\paragraph{Differentiable Optimization}
FlowMap minimizes a least-squares objective by comparing the optical flow generated from estimated depth, camera intrinsics, and poses against pre-computed optical flow and point tracks. 
Crucially, depth is parameterized using a neural network that maps input RGB frames to dense per-pixel depth maps, enabling consistent depth predictions across similar image patches. 
This approach helps FlowMap leverage geometric information robustly, even when patches are poorly constrained due to minimal motion or noise in correspondences.

Camera poses in FlowMap are computed analytically as the solution to an orthogonal Procrustes problem, aligning pairs of consecutive depth maps transformed by optical flow correspondences. 
This pose solver is differentiable, allowing gradient information to flow back into the depth network and correspondence weighting module.

\paragraph{Intrinsic Parameter Estimation}
FlowMap introduces a differentiable focal length estimator, softly selecting camera intrinsics from a set of candidate focal lengths based on optical flow consistency. 
After an initial soft selection stage, it switches to direct regression for focal length, benefiting from improved initialization and robust optimization.

\paragraph{Point Tracks for Robustness}
In addition to dense optical flow, sparse point tracks spanning longer sequences are employed. 
These tracks minimize drift over extended trajectories by providing long-term geometric consistency.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/related_work/flowmap_pipeline.png}
    \caption{FlowMap pipeline overview \cite{smith24flowmap}}.
    \label{fig:flowmap}
\end{figure}

\paragraph{Correspondence Loss}
The Correspondence Loss in FlowMap is a Camera-Induced Flow Loss, which use known correspondences to compute the reprojection error. 
To compute the loss, a points $\mathbf{u_i}$ is unprojected into the 3D space using the estimated depth map $\mathbf{D_i}$ and camera intrinsics $\mathbf{K_i}$, yielding a 3D point $\mathbf{X_i}$.
This point $\mathbf{X_i}$ is transfomed via the estimated relative camera pose $\mathbf{P_{ij}}$ to the second camera frame $\mathbf{I_j}$, where it is projected back into the second image plane.
The reprojection error is then computed as the difference between the projected point $\mathbf{u_j}$ and the known correspondence of $\mathbf{u_i}$ in the second image.


\paragraph{Comparison to Traditional SfM}
Unlike conventional methods, FlowMap produces dense depth estimates rather than sparse 3D points, a critical advantage for tasks like novel view synthesis. 
Empirical evaluations demonstrate that FlowMap achieves reconstruction quality comparable to COLMAP on standard benchmarks, such as Tanks \& Temples, Mip-NeRF 360, CO3D, and LLFF, especially when combined with downstream rendering approaches like Gaussian Splatting.

However, FlowMap does exhibit limitations. It relies heavily on accurate off-the-shelf correspondences, struggles with rotation-dominant trajectories, and can suffer from local minima such as hollow-face geometry inversions. 
Further, while it excels in scenarios involving continuous video data, it currently lacks mechanisms for processing unstructured image collections typical in classical SfM.

\section{Ace Zero}\label{sec:acezero}

AceZero \cite{brachmann2024acezero} introduces an innovative approach to Structure-from-Motion (SfM) by framing it as an incremental application and refinement of a learned visual relocalizer. 
This method diverges from conventional feature-matching SfM frameworks by employing scene coordinate regression, enabling the construction of implicit neural scene representations directly from unposed image collections without relying on local feature matching.

\paragraph{Scene Coordinate Regression}
At the core of AceZero is a scene coordinate regression method, a learning-based technique that regresses the coordinates of 3D scene points directly from 2D image locations. 
This bypasses the need for traditional feature extraction and matching, significantly streamlining the process and potentially reducing error propagation associated with matching failures. Unlike other neural reconstruction approaches, AceZero does not require camera pose priors or sequentially ordered inputs, enhancing flexibility and applicability.

\paragraph{Incremental Learning of Visual Relocalizer}
AceZero iteratively refines a visual relocalizer model, initially starting from a single seed image with a known identity pose. 
This model incrementally registers additional views, continuously improving its scene understanding and pose estimation capabilities. 
Through iterative learning and refinement, AceZero can robustly scale to datasets comprising thousands of images.

\paragraph{Advantages and Limitations}
AceZero demonstrates competitive pose accuracy compared to classical feature-based SfM methods, as shown through evaluations on standard benchmarks, including novel view synthesis experiments. 
One significant advantage is its ability to implicitly represent scenes using neural networks, potentially capturing complex scene structures better than discrete point clouds.

However, as a learning-based approach, AceZero is reliant on sufficient training data to generalize well across various scenes and environments. 
The method's performance might degrade in scenarios with insufficient data coverage or highly repetitive visual textures. Furthermore, the computational complexity associated with neural training and inference could present scalability challenges, particularly in real-time or near-real-time applications.

In summary, AceZero presents an exciting direction for neural-based SfM, promising improved robustness and implicit scene understanding capabilities, though practical considerations around training complexity and scalability must be carefully managed.

\section{Neural Radiance Fields (NeRF) for Novel View Synthesis}

Neural Radiance Fields (NeRF) \cite{mildenhall2020nerf} is a groundbreaking method for representing and rendering complex 3D scenes in a continuous and highly detailed manner. 
Originally proposed by Mildenhall et al., NeRF leverages neural networks to generate novel views from a sparse set of input images.

\subsection{Implicit Scene Representation}
Unlike traditional volumetric representations that discretize a scene into voxels, NeRF models a scene as a continuous function. 
This function is implemented as a fully connected neural network that maps a 5D input, comprising a 3D spatial coordinate $(x,y,z)$ and a 2D viewing direction $(\theta,\phi)$, to an output consisting of an RGB color vector and a scalar density value. 
To capture fine-grained details and high-frequency variations, the input coordinates are passed through a positional encoding function that transforms them into a higher-dimensional space.

\subsection{Volumetric Rendering and Ray Marching}
\paragraph{Ray Casting and Sampling:} For generating a novel view, a ray is cast from the camera's origin for each pixel. 
Multiple sample points are taken along the ray, and the network is queried at each point to obtain corresponding density and color values.  
\paragraph{Volume Rendering Integration:} The final pixel color is computed by integrating these samples using volumetric rendering principles. 
Each sample's contribution is weighted based on its density and position along the ray, in a manner similar to alpha compositing, where closer, more opaque samples influence the final rendered color more strongly.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/related_work/nerf_pipeline.png}
    \caption{NeRF Ray Sampling and Model Querying \cite{mildenhall2020nerf}.}
    \label{fig:nerf_pipeline}
\end{figure}

\subsection{Learning and Optimization Process}
\paragraph{Training Objective:} During training, the network's parameters are optimized by minimizing a photometric reconstruction loss (often the mean squared error) between the rendered images and the ground truth images.  
\paragraph{Differentiable Rendering:} The volumetric rendering process is fully differentiable, which allows gradients to flow through the entire pipeline. This enables the joint optimization of both scene geometry (via density) and appearance (via RGB color) within a unified framework.

\subsection{Novel View Synthesis}
Once the network is trained, NeRF can render the scene from arbitrary viewpoints, even ones not present in the training data. By varying the camera parameters (position and orientation) and applying the same ray marching and integration process, the network synthesizes photorealistic images. 
Moreover, the inclusion of the viewing direction helps capture view-dependent effects such as specular highlights and reflections.

\subsection{Advantages and Limitations}
\paragraph{Advantages:}
\begin{itemize}
    \item \textbf{High-Fidelity Rendering:} Produces photorealistic images with rich details.
    \item \textbf{Implicit Representation:} Avoids explicit geometry representations like voxels or meshes, enabling the capture of complex scene structures.
    \item \textbf{Differentiability:} Allows for joint optimization of geometry and appearance via end-to-end differentiable rendering.
\end{itemize}
\paragraph{Limitations:}
\begin{itemize}
    \item \textbf{Computational Complexity:} The training and rendering processes are computationally intensive.
    \item \textbf{Scene Specificity:} Standard NeRF models are trained per-scene and do not readily generalize to new scenes without retraining.
\end{itemize}

\subsection{Relation to Structure-from-Motion (SfM) Techniques}
SfM methods such as GLOMAP, VGGSfM, FlowMap, and Ace Zero primarily focus on reconstructing sparse or dense 3D point clouds and accurately estimating camera poses from image correspondences. 
In contrast, NeRF shifts the emphasis towards high-quality photorealistic rendering by learning a continuous implicit representation of both geometry and appearance. 
This approach provides additional benefits like capturing view-dependent effects, albeit at the cost of higher computational demands.
Since NeRF's need images as well as camera poses, it can be used in conjunction with SfM methods to provide a complete pipeline for 3D reconstruction and rendering.