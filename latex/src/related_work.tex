\chapter{Background and Related Work}\label{chap:related_work}

In this chapter, we provide a comprehensive overview of the foundational concepts and recent advancements relevant to Structure-from-Motion (SfM).
We begin by describing classical SfM techniques, highlighting traditional feature extraction, matching methods, and optimization strategies. 
Subsequently, we review emerging learning-based approaches, focusing on methods that leverage neural networks to enhance reconstruction accuracy, efficiency, and robustness. 
The chapter further delves into detailed discussions of specific state-of-the-art methods, including GLOMAP, VGGSfM, FlowMap, and AceZero, 
clarifying their claimed advantages and limitations compared to classical SfM pipelines like COLMAP.

\section{Classical SfM}
SfM approaches can be broadly classified into two categories: incremental and global.

\subsection{Incremental SfM}
Incremental Structure-from-Motion (SfM) follows a sequential approach, where the 3D reconstruction is built progressively by adding one image at a time. 
This method begins with an initial pair of images and expands the reconstruction iteratively by registering new images and refining the existing structure. 
The core steps of incremental SfM include feature extraction and matching, relative camera pose estimation, and bundle adjustment.

\paragraph{Feature Extraction and Matching}
The first step in incremental SfM involves detecting and describing key points in each image. 
Traditional methods rely on hand-crafted feature descriptors like SIFT \cite{Lowe2004DistinctiveIF} or ORB \cite{rublee2011orb}, 
which are designed to extract distinctive image features that are robust to changes in scale, rotation, and lighting conditions.

Once features are detected, they are compared across overlaping images to establish correspondences.
This is typically done using nearest-neighbor matching, where the descriptor of a feature in one image is compared to all features in another image to find the best match.
This process can be computationally expensive, especially for large datasets, and is often optimized using techniques like FLANN \cite{muja2009fast} or approximate nearest neighbor search.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/related_work/matches.jpg}
    \caption[Feature matching example]{Example of feature matching between two images. The lines connect matched features across the two images.}
    \label{fig:incremental_sfm}
\end{figure}


\paragraph{Relative Camera Pose Estimation}
Given the matched feature correspondences between image pairs, the next step is to estimate the motion of the camera between the two views.
This is typically done by computing the essential or fundamental matrix, which encapsulates the relative rotation and translation between the two camera poses.

This is achieved using epipolar geometry constraints, often solved via the five-point or eight-point algorithms within a RANSAC \cite{fischler1981random} framework to remove matches that are outliers.
The estimated relative camera pose allows for triangulation, reconstructing initial 3D points.

\paragraph{Bundle Adjustment}
As new images are added, the estimated structure and camera parameters can suffers from drift and inaccuracies.
Bundle adjustment is a key step in incremental SfM that refines the camera poses and 3D structure by minimizing the reprojection error.
Non-linear least squares solvers such as Levenberg-Marquardt or Ceres solver are commonly used to achieve this refinement. 
Incremental SfM benefits from high accuracy but suffers from scalability issues as the number of images increases.

\subsection{Global SfM}
In contrast to incremental approaches, global SfM processes all images simultaneously to compute a globally consistent 3D structure.
Instead of reapeting the costly bundle adjustment, global SfM methods estimate camera geometry for all input images at once.

A \emph{view graph} is constructed with all image pairs and their estimated relative poses.

The graph is then used to estimate camera intrinsics, as well as construction of a relative camera pose graph.
The relative camera pose graph is then used to perform \emph{rotation averaging} as well as \emph{translation averaging} \cite{Chatterjee2013, theia-manual, moulon2016openmvg}
Finally, with both global rotations and translations estimated, a global bundle adjustment is performed to refine the camera parameters and 3D point positions, ensuring a consistent and accurate reconstruction of the scene.
This approach allow for more scalable reconstructions.

\paragraph{However, three limitations remains:}
\begin{enumerate}
    \item \textbf{Colinear motion}: When the motion of the camera is colinear, it leads to degenerate reconstruction problem since the essential martrix only tells the direction of relative translation.
    This represent a challenge as such motion is frequently encountered in real-world scenarios (e.g., self-driving car or drone flying in a straight line).
    \item \textbf{Translation direction challenges}: The process of translation averaging is particularly sensitive to noise and outliers. 
    To accurately estimate the global camera position, triplet of relative translation directions are required. However, when these triplets form a skewed triangle, the translation averaging process becomes instable \cite{manam2023sensitivity}.
    \item \textbf{Camera intrinsics}: To decompose the relative two-view geometry into rotation and translation, the camera intrinsics must be known and accurate.
    If the intrinsics matrix $\mathbf{K}$ is not accuratly estimated, it can lead to large errors in the translation direction.
\end{enumerate}

\section{COLMAP}\label{sec:colmap}
COLMAP \cite{schoenberger2016sfm} is an open-source, modular SfM and Multi-View Stereo (MVS) system introduced by \textit{Schönberger and Frahm} in 2016. 
It implements a complete incremental reconstruction pipeline while offering dense reconstruction via PatchMatch stereo. COLMAP is designed for extensibility and high performance, and has become a widely adopted baseline in both academic and industrial 3D reconstruction tasks.

\paragraph{Feature Extraction and Matching}
COLMAP employs SIFT \cite{Lowe2004DistinctiveIF} feature detection and description for robust feature extraction. 
Since COLMAP is easily modular, RootSIFT \cite{arrandjelovic2012three} or more recent learning-based method such as SuperPoint \cite{detone18superpoint}, can be optionally used for improved matching robustness.
By default, exhaustive matching is used for small image sets, while for larger collections an image retrieval stage using a vocabulary-tree accelerates the selection of candidate image pairs. 
All putative matches are filtered via two-view geometric verification using fundamental or essential matrices estimated with RANSAC \cite{fischler1981random}.

\paragraph{Incremental Reconstruction Pipeline}
The incremental SfM pipeline proceeds as follows:
\begin{enumerate}
  \item \textbf{Initialization.} A well-conditioned image pair is chosen based on match count and baseline, and an initial two-view reconstruction is computed.
  \item \textbf{Image Registration.} Each subsequent image is registered by solving the Perspective-n-Point problem with RANSAC to estimate its pose relative to the growing 3D model.
  \item \textbf{Triangulation.} New 3D points are created by triangulating feature tracks that span at least two registered views.
  \item \textbf{Local Bundle Adjustment.} After each registration, a local bundle adjustment refines the poses of the newly added image and its neighbors to minimize reprojection error.
  \item \textbf{Global Bundle Adjustment.} Periodically, a full BA over all cameras and points is run to eliminate drift and optimize the entire reconstruction resulting in an amortized linear run-time.
\end{enumerate}

% \subsection{Dense Reconstruction}
% After sparse SfM, COLMAP can perform dense MVS via a GPU-accelerated PatchMatch stereo algorithm. For each reference image, a depth and normal map is estimated by propagating and refining plane hypotheses, followed by fusion into a global dense point cloud or mesh. This approach combines high accuracy with real-time performance for hundreds of views.

% \paragraph{Implementation Details}
% \begin{itemize}
%   \item \textbf{Solver.} All optimization (BA, stereo) leverages the Ceres Solver for efficient non-linear least squares.
%   \item \textbf{Data Structures.} COLMAP uses a database backend to store features, matches, and camera parameters, enabling on-disk handling of large datasets.
%   \item \textbf{Extensibility.} Each pipeline stage is exposed as a command-line tool or library API, allowing users to swap in custom feature detectors, matching strategies, or outlier filters.
% \end{itemize}

Overall, COLMAP provides a robust, end-to-end reconstruction framework that balances accuracy, scalability, and ease of integration, making it the de facto open-source choice for SfM and 3D reconstruction.

\section{GLOMAP}\label{sec:glomap}
GLOMAP \cite{pan2024glomap} presented by \textit{L. Pan et al.} is a recent global Structure-from-Motion method that aims to overcome the challenges of global SFM. It can deal with unknown camera intrinsics as well as colinear motion.

\paragraph{Feature Extraction and Matching}
The pipeline rely of COLMAP features extraction and matching, which use SIFT \cite{Lowe2004DistinctiveIF} for features extraction and has several option for matching: 
\begin{itemize}
    \item \textbf{Exhaustive}: All images are matched against each other, which is computationally expensive but provides the most accurate matches.
    \item \textbf{Sequential}: Matches are computed in a sequential manner, which is more efficient than exhaustive matching but requires sequential images.
    \item \textbf{Vocabulary Tree}: A Bag-of-Words images retrival approach \cite{schoenberger2016vote} that allows to find overlaping images in a large dataset and match them.
\end{itemize}
\textbf{Note:} The use of library such as Hloc \cite{sarlin2019coarse} can serve as a drop-in replacement for the feature extraction and matching step.

\paragraph{Camera Pose Estimation and Triangulation}
Since GLOMAP is a global SfM method, it seeks to estimate all the camera rotation and translation parameters simultaneously.
The global camera pose estimation is performed usually performed by a combination of rotation averaging and translation averaging. 
However, due to noise and outliers as well as scale ambiguity, translation averaging is particularly challenging.

GLOMAP introduces a novel approach and perform rotation averaging and a steps called \emph{Global positioning} instead of translation averaging. \\
Global positioning is the key step for robustness and perform directly a joint camera and point position estimation. 
Employing the BATA loss \cite{zhuang2019baselinedesensitizingtranslationaveraging} with a bounded reprojection error in the range $[0, 1]$. 
This allows the optimization process to converge quickly and prevent outliers from impacting the results.

By avoiding the translation averaging step, GLOMAP is able to handle colinear motion and better handle outliers.

Finally, global bundle adjustment is performed to refine the camera parameters and 3D point positions, ensuring a consistent and accurate reconstruction of the scene.

The authors claim superior performance and accuracy compared to traditional global SfM methods, and on par with COLMAP, while being significantly faster.

A limitation of GLOMAP remains. If the scene has a rotational symmetry, the rotation averaging step collapses, making it a degenerate case for GLOMAP.

\section{VGGSfM}\label{sec:vggsfm}
Visual Geometry Grounded Deep Structure From Motion (VGGSfM) \cite{wang2023vggsfm} presented by \textit{Wang et al.} introduces a novel, fully differentiable pipeline for Structure-from-Motion (SfM) that harnesses the potential of deep learning. The authors propose a unified framework that integrates the entire SfM pipeline into a function $f_{\theta}$, parameterized by $\theta$, which maps a set of images $\mathcal{I}$ to camera parameters $\mathcal{P}$ and a 3D point cloud $\mathcal{X}$. Here, $\theta$ is a learnable parameter, optimized by minimizing a loss function $\mathcal{L}$.

In its original formulation, $f_\theta$ can be decomposed into four stages: (1) Point Tracking, (2) Camera Estimator, (3) Triangulator, and (4) Bundle Adjustment.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/related_work/vggsfm_pipeline.png}
    \caption{VGGSfM pipeline overview \cite{wang2023vggsfm}}.
    \label{fig:vggsfm}
\end{figure}

\paragraph{Point Tracking.}
Unlike traditional SfM methods that compute pairwise correspondences and chain them into multi-image tracks, VGGSfM employs a deep feed-forward tracker to predict pixel-accurate multi-image correspondences directly from the input images. 
It typically selects a set of query points in one image (e.g.\ using SuperPoint \cite{detone18superpoint}, SIFT \cite{Lowe2004DistinctiveIF}, ALIKED \cite{Zhao2023ALIKED}, or a combination) and extracts their feature descriptors. These descriptors are matched across all other images using a multi-scale cost volume, encoding their similarity. 
A Transformer attends to all input frames jointly, predicting the 2D location of each query point in every view along with its visibility and confidence. 
In contrast to pairwise matching that relies on incremental chaining, VGGSfM's direct multi-image tracking reduces both complexity and drift errors.

\paragraph{Camera Estimator and Triangulator.}
The initial camera estimator and triangulator stages use deep Transformers to generate an initial estimate of the camera parameters $\mathcal{P}$ and the 3D points $\mathcal{X}$, as opposed to relying on rotation/translation averaging or incremental registration. 
In the original paper's pipeline, the network could leverage track features as well as image features to produce camera poses. All cameras and points are thus registered collectively, ensuring differentiability and bypassing many incremental SfM complexities.

\paragraph{Bundle Adjustment.}
Finally, the pipeline employs a differentiable second-order Levenberg-Marquardt optimizer from the Theseus library \cite{pineda2022theseus} to refine both the camera parameters and 3D points by minimizing reprojection error. 
Because bundle adjustment (BA) is fully differentiable, the entire SfM pipeline can be trained end-to-end, unlike in traditional approaches that use non-differentiable solvers.

\paragraph{Comparison with Global SfM}
VGGSfM is related to global SfM approaches but differs in three crucial aspects:
\begin{itemize}
    \item It learns to track points directly from the images, rather than performing pairwise matching.
    \item It uses a deep network to regress camera poses in a single pass, rather than rotation/translation averaging.
    \item It employs differentiable bundle adjustment to refine parameters in an end-to-end manner.
\end{itemize}

\subsection*{VGGSfM v1.1 Updates}
Starting from v1.1, the VGGSfM implementation separates the camera predictor from the point-tracking stage. 
Concretely, the camera predictor now relies \emph{only} on image features rather than requiring track features. 
This design choice arose in response to users feedback:
\begin{itemize}
    \item \textbf{Faster Pose Estimates.} Some scenarios benefit from real-time or near-real-time camera pose estimation without waiting for the (comparatively slower) track prediction.
    \item \textbf{Flexibility.} Users can opt to generate approximate camera poses quickly and refine them later (if desired) with track-based bundle adjustment.
\end{itemize}
This change does slightly reduce the initial pose accuracy—because track features are no longer used for the first pose guess—but in practice, errors remain small enough (e.g.\ $\approx 3^\circ$ mean rotational error and $6\%$ translational error) \cite{VGGSFM_GitHubIssue}. 
These errors can be effectively corrected via the subsequent BA stage if a high-accuracy reconstruction is needed.


While VGGSfM demonstrates robust performance on several benchmarks such as Co3D \cite{reizenstein21co3d}, IMC \cite{Jin_2020}, and ETH3D \cite{schoeps2017cvpr}, the authors acknowledge it does not (yet) handle large-scale datasets (thousands of frames) as readily as classical SfM pipelines. Ongoing work (VGGSfM v2) aims to address these scalability challenges while maintaining the end-to-end differentiability and strong performance.

\section{FlowMap}\label{sec:flowmap}
FlowMap \cite{smith24flowmap} presented by \textit{C. Smith et al.} is a recently introduced Structure-from-Motion method leveraging an end-to-end differentiable approach for estimating camera poses, intrinsics, and dense per-frame depth maps from video sequences. 
Unlike classical SfM methods such as COLMAP, FlowMap does not rely on explicit 3D point triangulation from sparse correspondences. 
Instead, it formulates SfM as a gradient-descent optimization problem, supervised purely by off-the-shelf optical flow and sparse point tracking correspondences.

\paragraph{Differentiable Optimization}
FlowMap minimizes a least-squares objective by comparing the optical flow generated from estimated depth, camera intrinsics, and poses against pre-computed optical flow and point tracks. 
Crucially, depth is parameterized using a neural network that maps input RGB frames to dense per-pixel depth maps, enabling consistent depth predictions across similar image patches. 
This approach helps FlowMap leverage geometric information robustly, even when patches are poorly constrained due to minimal motion or noise in correspondences.

Camera poses in FlowMap are computed analytically as the solution to an orthogonal Procrustes problem, aligning pairs of consecutive depth maps transformed by optical flow correspondences. 
This pose solver is differentiable, allowing gradient information to flow back into the depth network and correspondence weighting module.

\paragraph{Intrinsic Parameter Estimation}
FlowMap introduces a differentiable focal length estimator, softly selecting camera intrinsics from a set of candidate focal lengths based on optical flow consistency. 
After an initial soft selection stage, it switches to direct regression for focal length, benefiting from improved initialization and robust optimization.

\paragraph{Point Tracks for Robustness}
In addition to dense optical flow, sparse point tracks spanning longer sequences are employed. 
These tracks minimize drift over extended trajectories by providing long-term geometric consistency.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/related_work/flowmap_pipeline.png}
    \caption{FlowMap pipeline overview \cite{smith24flowmap}}.
    \label{fig:flowmap}
\end{figure}

\paragraph{Correspondence Loss}
The Correspondence Loss in FlowMap is a Camera-Induced Flow Loss, which use known correspondences to compute the reprojection error. 
To compute the loss, a points $\mathbf{u_i}$ is unprojected into the 3D space using the estimated depth map $\mathbf{D_i}$ and camera intrinsics $\mathbf{K_i}$, yielding a 3D point $\mathbf{X_i}$.
This point $\mathbf{X_i}$ is transfomed via the estimated relative camera pose $\mathbf{P_{ij}}$ to the second camera frame $\mathbf{I_j}$, where it is projected back into the second image plane.
The reprojection error is then computed as the difference between the projected point $\mathbf{u_j}$ and the known correspondence of $\mathbf{u_i}$ in the second image.


\paragraph{Comparison to Traditional SfM}
Unlike conventional methods, FlowMap produces dense depth estimates rather than sparse 3D points, a critical advantage for tasks like novel view synthesis. 
Empirical evaluations demonstrate that FlowMap achieves reconstruction quality comparable to COLMAP on standard benchmarks, such as Tanks \& Temples, Mip-NeRF 360, CO3D, and LLFF, especially when combined with downstream rendering approaches like Gaussian Splatting.

However, FlowMap does exhibit limitations. It relies heavily on accurate off-the-shelf correspondences, struggles with rotation-dominant trajectories, and can suffer from local minima such as hollow-face geometry inversions. 
Further, while it excels in scenarios involving continuous video data, it currently lacks mechanisms for processing unstructured image collections typical in classical SfM.

\section{AceZero}\label{sec:acezero}
AceZero \cite{brachmann2024acezero} presented by \textit{E. Brachmann et al.} introduces an innovative approach to Structure-from-Motion (SfM) by framing it as an incremental application and refinement of a learned visual relocalizer. 
This method diverges from conventional feature-matching SfM frameworks by employing scene coordinate regression, enabling the construction of implicit neural scene representations directly from unposed image collections without relying on local feature matching.

\paragraph{Scene Coordinate Regression}
At the core of AceZero is a scene coordinate regression method \cite{brachmann2023ace}, a learning-based technique that regresses the coordinates of 3D scene points directly from 2D image locations. 
This bypasses the need for traditional feature extraction and matching, significantly streamlining the process and potentially reducing error propagation associated with matching failures. Unlike other neural reconstruction approaches, AceZero does not require camera pose priors or sequentially ordered inputs, enhancing flexibility and applicability.

\paragraph{Incremental Learning of Visual Relocalizer}
AceZero iteratively refines a visual relocalizer model, initially starting from a single seed image with a known identity pose. 
This model incrementally registers additional views, continuously improving its scene understanding and pose estimation capabilities. 
Through iterative learning and refinement, AceZero can robustly scale to datasets comprising thousands of images.

\paragraph{Advantages and Limitations}
AceZero demonstrates competitive pose accuracy compared to classical feature-based SfM methods, as shown through evaluations on standard benchmarks, including novel view synthesis experiments. 
One significant advantage is its ability to implicitly represent scenes using neural networks, potentially capturing complex scene structures better than discrete point clouds.

However, as a learning-based approach, AceZero is reliant on sufficient training data to generalize well across various scenes and environments. 
The method's performance might degrade in scenarios with insufficient data coverage or highly repetitive visual textures. 
Furthermore, the scene coordinate regressor struggles with large-scale scenes, as well as big lighting changes such as day-night transitions.
AceZero also consider the camera intrinsics to be shared across all images, which may not be suitable for all datasets.

In summary, AceZero presents an exciting direction for neural-based SfM, promising improved robustness and implicit scene understanding capabilities, though it may face challenges in generalization and scalability compared to traditional methods.

\section{Gaussian Splatting for Novel View Synthesis}\label{sec:gaussian_splatting}
Gaussian Splatting \cite{kerbl20233dgaussiansplattingrealtime} is a recently introduced technique for rendering 3D scenes in real time by representing them with a set of continuous Gaussian functions.
Much like NeRF \cite{mildenhall2020nerf}, it can generate novel views from sparse images, but it departs from NeRF's dense sampling strategy and instead relies on a compact set of Gaussian “splats” to capture scene geometry and appearance.

\subsection{Scene Representation}
Instead of relying on dense voxel grids or explicit meshes, Gaussian Splatting represents a 3D scene as a collection of Gaussian functions. 
Each Gaussian splat is defined by its mean, which specifies the 3D center; its covariance, a $3 \times 3$ matrix that encodes the shape, scale, and orientation (often in an anisotropic manner); its associated RGB color; and a weight that modulates its contribution in the scene. 
This representation creates a continuous model of the scene where neighboring Gaussians naturally blend, leading to smooth visual transitions.

Formally, each Gaussian $g_i(\mathbf{x})$ is expressed as
\begin{equation}
    g_i(\mathbf{x}) = \sigma(\alpha_i) \exp\left(-\frac{1}{2} \bigl(\mathbf{x} - \boldsymbol{\mu}_i\bigr)^\top \Sigma_i^{-1} \bigl(\mathbf{x} - \boldsymbol{\mu}_i\bigr)\right),
\end{equation}

where $\boldsymbol{\mu}_i$ is the 3D center, $\Sigma_i$ is the covariance matrix, and $\sigma(\alpha_i)$ is a sigmoid function that modulates the Gaussian's opacity based on its weight $\alpha_i$.

\subsection{Volumetric Rendering and Ray Marching}
\paragraph{Ray Casting and Sampling:}
To render a novel view, Gaussian Splatting conceptually follows a ray-casting procedure akin to NeRF.
A ray is cast from each pixel, intersecting the cloud of Gaussians in 3D.
However, instead of sampling many points along the ray, the algorithm computes the projected contribution of each Gaussian onto the ray.
In practice, these Gaussians project to ellipses in the image, often referred to as “splats.”

\paragraph{Volume Rendering Integration:}
Like traditional volumetric rendering, the final color of each pixel is obtained by integrating the contributions of the Gaussians encountered along that ray.
Closer, high-density Gaussians have a stronger influence on the pixel color, while distant or low-density Gaussians are more transparent.
This process can be viewed as a weighted alpha-compositing step:

% \begin{equation}
%     \text{Color}(\mathbf{r}) = \sum_{i} T_{i-1} \,\alpha_i \,g_i(\mathbf{r}),
% \end{equation}

\begin{equation}
    \text{Color}(\mathbf{r}) = \sum_{i} \prod_{j=1}^{i-1}(1 - \alpha_j) \,\alpha_i \,g_i(\mathbf{r}),
\end{equation}

where $\alpha_i$ is the opacity $i$-th Gaussian, and $\prod_{j=1}^{i-1}(1 - \alpha_j)$ is the accumulated transmittance up to Gaussian $i-1$.

Because each Gaussian has a continuous extent (defined by its covariance), the representation is smooth and captures fine scene details without requiring dense point sampling.

\subsection{Learning and Optimization Process}
\paragraph{Training Objective:}
During training, a photometric loss between rendered images and ground truth images drives the optimization of each Gaussian's parameters (mean, covariance, color, and weight).
Typically, a mean squared error or a combination of L1 and SSIM loss is used to measure the difference between the rendered and target images.
The optimization process adjusts the Gaussian parameters to minimize this loss, effectively learning the scene representation.

\paragraph{Differentiable Rendering:}
Crucially, the Gaussian Splatting formulation is fully differentiable.
By computing analytic derivatives of each Gaussian's projection and integration, gradients can flow back through the splatting process.
This means the positions, shapes, and colors of the Gaussians are refined in an end-to-end manner, allowing the model to converge toward an accurate and compact representation of the scene.

\subsection{Novel View Synthesis}
Once trained, the collection of Gaussians can be used to render the scene from any new viewpoint by projecting them onto the new camera plane and compositing their contributions.
Despite the absence of a dense voxel or mesh structure, Gaussian Splatting achieves high-quality visual fidelity while often being more computationally efficient than sampling thousands of points along each ray (as done in many NeRF-style methods).
It also naturally handles partially transparent or semi-opaque regions because each Gaussian's opacity contributes smoothly during splatting.

\subsection{Advantages and Limitations}
\paragraph{Advantages:}
\begin{itemize}
    \item \textbf{Efficiency:} Fewer samples per ray are needed compare to NeRFs because each Gaussian describes a continuous region in space.
    \item \textbf{Smooth Blending:} Overlapping Gaussians create soft transitions, reducing aliasing and flickering in rendered views.
    \item \textbf{Differentiable Splatting:} The rendering process is end-to-end trainable, enabling data-driven optimization.
    \item \textbf{Real-Time Potential:} With the right GPU-based implementations, Gaussian Splatting can achieve near-real-time rendering performance.
\end{itemize}

\paragraph{Limitations:}
\begin{itemize}
    \item \textbf{Sensitivity to Initialization:} Selecting initial Gaussian parameters can be challenging; poor initialization may lead to suboptimal solutions.
    \item \textbf{Complex Scenes:} Highly detailed or large-scale scenes may require many Gaussians, increasing both memory usage and render time.
    \item \textbf{Dependence on Pose Accuracy:} As with NeRF, reliable camera poses (or a reliable pose optimization stage) are essential for achieving high-quality results. That is why accurate SfM is crucial.
\end{itemize}

\subsection{Relation to Structure-from-Motion (SfM) Techniques}
Like NeRF, Gaussian Splatting requires knowledge of camera intrinsics and extrinsics to project Gaussians correctly onto the image plane.
SfM pipelines such as GLOMAP, VGGSfM, FlowMap, and AceZero can provide these camera poses from unstructured image sets or video.
Once the poses are available, Gaussian Splatting can be trained or refined using the resulting images.
This synergy allows for:
\begin{itemize}
    \item \textbf{Accurate Geometry Initialization:} SfM's 3D points or camera parameters help position Gaussians in the correct locations.
    \item \textbf{Improved Novel View Synthesis:} The continuous Gaussian representation can fill in details and handle transparent or semi-transparent regions better than sparse point-based methods alone.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/gs_splat_pipeline.png}
    \caption{Gaussian Splatting pipeline overview \cite{kerbl20233dgaussiansplattingrealtime}.}
    \label{fig:gaussian_splatting}
\end{figure}

Overall, Gaussian Splatting offers an efficient and differentiable avenue for novel view synthesis, complementing existing SfM reconstructions by generating high-fidelity renders at interactive speeds.