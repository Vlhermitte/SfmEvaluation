\chapter{Experiments}\label{chap:experiments}

In this section, we present the experimental setup and results of our evaluation of the different methods for 3D reconstruction and novel view synthesis. 
We compare the performance of VGGSfM, GLOMAP, AceZero, and FlowMap on selected datasets, including ETH3D, LaMAR, MipNeRF360, and Tanks and Temples.

\section{Experimental Setup}\label{sec:experimental-setup}
\subsubsection{Hardware}
All methods were evaluated on a HPC server, equiped with NVIDIA Tesla V100 GPUs (32 GB). Only a single V100 was used to ensure a level playing field as VGGSfM and AceZero simply don't offer multi-GPU support.
GLOMAP, which mirrors COLMAP's GPU-accelerated feature extractor and matcher, but its core mapping (bundle adjustment, graph construction, etc.) remains CPU-only in the current codebase. 
For that stage we provisioned 32 CPU threads on an Intel Xeon Scalable Gold 6150; in practice, however, CPU-core count has negligible impact on the other pipelines, since they execute end-to-end on the GPU.

\subsubsection{Software}
We made sure that all methods would produce the results in a COLMAP-compatible format (\textit{images.bin, cameras.bin, points3D.bin}) to make it easier to handle the data and to compare the results.
A Python-based pipeline was created and used pycolmap to handle the data throughout the process.

\subsubsection{Gaussian Splatting parameters}
The gaussian splatting was trained using 30 000 iterations and camera poses optimization enabled.

If a method use a camera model that handle lens distortion, we undistort the images using the OpenCV library \cite{opencv_library} before feeding them to the Gaussian Splatting pipeline.

\subsection{SfM methods parameters}
\paragraph{COLMAP}
It was run with a \textit{RADIAL} camera model and the \textit{Exhaustive Matcher} feature matcher unless specified otherwise.

The COLMAP version used for this experiment is 3.11.1.

\paragraph{VGGSfM}
VGGSfM was run with camera type set to \textit{SIMPLE RADIAL} and the default image keypoints and descriptors extractor \textit{ALIKED} \cite{Zhao2023ALIKED}.

We did however reduce the number of predicted tracks from 163.840 to 40.960, as well as the number of triangulated tracks from 819.200 to 204.800 
in order to reduce the memory usage since VGGSfM is indended to run on a a larger GPU with higher memory.

For this experiment, we use VGGSfM 2.0.

\paragraph{GLOMAP}
GLOMAP feature extractor was set to COLMAP's feature extractor \textit{SIFT} \cite{Lowe2004DistinctiveIF} and feature matcher to COLMAP's \textit{Exhaustive Matcher} unless specified otherwise.
We use the \textit{RADIAL} camera model.

For this experiment, we use GLOMAP version 1.0.0.

\paragraph{AceZero}\label{sec:acezero-parameters}
AceZero was run with the default parameters, which include the use of \textit{SIMPLE PINHOLE} camera model. 
This could represente a limitation for the method as it is not able to handle lens distortion.

AceZero provides a useful confidence score. Following the authors' recommendation, if a pose has a confidence score lower than 1000, we consider the pose as unreliable and set it to an unregistered image.

\paragraph{FlowMap}
To run FlowMap, we used the provided pretrained model which was trained on CO3D \cite{reizenstein21co3d}, Real Estate 10K, and KITTI \cite{geiger2012kitti} datasets.
This pre-training claims to achieve faster convergence and slightly improves the camera poses estimation.

While the pre-training has been done using GMFlow \cite{xu2022gmflow} for optical flow, we used the default \textit{RAFT} \cite{teed2020raft} optical flow model for the evaluation.

FlowMap also includes a low-memory mode, which reduces the resolution at which optical flow is computed. 
For scenes containing more than 180 images, we enable this mode by setting the corresponding parameter to \textit{True}. 
Through empirical evaluation on our GPU configuration, we found that 180 images serves as a practical threshold to prevent out-of-memory issues during processing.

\subsection{Datasets}
We evaluate the performance of different methods in estimating camera poses using the ETH3D and LaMAR datasets, both of which provide ground truth pose annotations.

\paragraph{LaMAR}
For the LaMAR dataset, we do not rely on the provided tools, as they are not well-suited for methods such as AceZero and FlowMap. Instead, we execute each Structure-from-Motion (SfM) pipeline manually on individual iOS sessions. 
Each session contains a large number of images and was captured in a sequential manner, which allows certain methods to take advantage of sequential processing.

In particular, we run the following methods configuration on the LaMAR dataset:

\begin{itemize}
    \item \textbf{COLMAP and GLOMAP:} These methods benefit from using the \textit{Sequential Matcher}, which significantly reduces computation time compared to the \textit{Exhaustive Matcher}.
    \item \textbf{VGGSfM:} Can operates in sequential mode using a sliding window approach, which lowers memory requirements and enables the reconstruction of longer sequences. 
                           But we found during our experiments that the sliding windows got stuck in an infinite loop while trying to find valid frames for the LaMAR dataset. Or the bundle adjustment would fail, resulting in a crash.
                           Thus, we do not leverage the sequential mode for this experiment.
    \item \textbf{FlowMap:} Naturally designed for sequential processing; no changes in configuration are necessary.
    \item \textbf{AceZero:} Lacks support for sequential processing. We therefore apply the same parameters as detailed in Section~\ref{sec:acezero-parameters}.
\end{itemize}

The novel view synthesis results are evaluated on the datasets ETH3D, MipNeRF360 and Tanks and Temples. 
As shown in section \ref{sec:experimental-results}, we use MipNeRF360 with an image resolution downscaled from the original $4949 \times 3286$ to $2473 \times 1643$

\section{Experimental Results}\label{sec:experimental-results}

\subsection{Camera Pose Estimation Results}\label{sec:camera-pose-estimation-results}

\subsubsection{ETH3D}\label{sec:eth3d-evaluation-results}
\input{src/poses/traj_results_eth3d.tex}

\subsubsection{LaMAR}\label{sec:lamar-evaluation-results}
\input{src/poses/traj_results_lamar.tex}

\subsection{Novel View Synthesis Results}\label{sec:gs-evaluation-results}
\input{src/nvs/nvs_results_eth3d.tex}
\input{src/nvs/nvs_results_mp360.tex}
\input{src/nvs/nvs_results_t2.tex}

\paragraph{Take-away.}
Overall, GLOMAP and COLMAP remains the top performers for the novel view synthesis task, 
AceZero and VGGSfM also produced honorable results on MipNeRF360 and Tanks and Temples, but they are not able to produce good results on ETH3D.
FlowMap remains the worst perfroming method and results shown by the authors do not match our results.

\subsection{Time And Memory Evaluation}\label{sec:time-and-memory-evaluation}
\input{tables/timings/eth3d_timings.tex}

\input{tables/timings/mipnerf360_timings.tex}

\input{tables/timings/tanksandtemples_timings.tex}
\input{tables/timings/tanksandtemples_reduced_timings.tex}

Tables [\ref{tab:time_performance_ETH3D}, \ref{tab:time_performance_MipNerf360}, \ref{tab:time_performance_TanksAndTemples}, \ref{tab:time_performance_TanksAndTemples reduced}]
clearly shows that GLOMAP is the fastest method by a significant margin.
COLMAP remains fast for small datasets compared to VGGSfM but quickly becomes out-paced for larger datasets.
AceZero is the slowest on average, but it is worth noting that as the number of images increases, the time performance of AceZero improves.
FlowMap running time is faster than AceZero on ETH3D but slower on the reduced version of Tanks and Temples.
However, given the results shown in section \ref{sec:camera-pose-estimation-results} and \ref{sec:gs-evaluation-results}, Flowmap's time performance is not worth the trade-off in accuracy.

\input{tables/timings/lamar_cab_timings.tex}
\input{tables/timings/lamar_hge_timings.tex}
\input{tables/timings/lamar_lin_timings.tex}

The LaMAR dataset tells the same story. GLOMAP demonstrates faster convergence than the other methods. 
The reason COLMAP shows very few results, is explained bellow in section \ref{sec:note}. However, \cite{pan2024glomap} and \cite{brachmann2024acezero} report that COLMAP becomes much slower when high number of images are involved. 
Which is to be expected from a incremental SfM pipeline.

For LIN \ref{tab:time_performance_LaMAR LIN}, GLOMAP seems more consistent with the results from [\ref{tab:time_performance_ETH3D}, \ref{tab:time_performance_MipNerf360}, \ref{tab:time_performance_TanksAndTemples}, \ref{tab:time_performance_TanksAndTemples reduced}].

AceZero runtime depends largly on the scenes, the number of iterations depends on the scene complexity.
Indeed, AceZero's paper \cite{brachmann2024acezero} states that the method is designed to be efficient for large datasets but the runtime complexity depends on two factors: the number of images and the spatial distribution of cameras.
In the worst case, the complexity is $O(n^2)$ where n is the number of images (e.g a camera trajectory without intersection or loops). 
The best case is $\Omega(n)$ when the cameras are well distributed in space.
The authors also claim much faster performance than COLMAP, on very large datasets, (e.g. 10k+ images with dense coverage of the scenes). 
But we did not have the opportunity to test this claim due to time constraints.

VGGSfM and FlowMap results are to be taken with caution as they only ran on very few sessions due to GPU memory constraints.

\paragraph{Take-away.}
GLOMAP is the fastest method for most datasets, and COLMAP remains a good option. 
\textit{L. Pan et al} \cite{pan2024glomap} reports that the method largely outperforms COLMAP on large datasets (a few thousands images) during the camera pose estimation and 3D mapping tasks.
VGGSfM's results are satisfactory for small to medium sized datasets.
FlowMap and AceZero remains the slowest

\subsubsection{Note}\label{sec:note}
Any scene not presented for COLMAP, are due to COLMAP unable to find a good initial image pair to start the reconstruction. 
In that case we report the time and memory usage when COLMAP manage to register at least a good amount of the images, 
otherwise it would be taking into account very small models (e.g. 10 images), which would report very fast runtime in the tables above and skew the results.